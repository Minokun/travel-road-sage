"""
æœç´¢æœåŠ¡æ¨¡å—
åŒ…å«å°çº¢ä¹¦æœç´¢å’Œ DuckDuckGo æœç´¢
"""
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
import sys
import os
import logging
from urllib.parse import urlencode

# æ·»åŠ  utils ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))
from utils.ddgs_utils import search_ddgs
from app.config import settings

logger = logging.getLogger(__name__)


class SearchService:
    """æœç´¢æœåŠ¡"""
    
    def __init__(self):
        self.xiaohongshu_base_url = "https://www.xiaohongshu.com/search_result"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
    
    async def search_unsplash_image(self, destination: str) -> Optional[str]:
        """
        ä½¿ç”¨ Unsplash API æœç´¢æ—…è¡Œå›¾ç‰‡
        
        Args:
            destination: ç›®çš„åœ°åç§°
            
        Returns:
            å›¾ç‰‡ URL æˆ– None
        """
        if not settings.UNSPLASH_ACCESS_KEY:
            logger.warning("Unsplash API Key æœªé…ç½®")
            return None
        
        try:
            url = "https://api.unsplash.com/search/photos"
            params = {
                "query": f"{destination} travel scenery landscape",
                "per_page": 5,
                "orientation": "landscape",
                "content_filter": "high"
            }
            headers = {
                "Authorization": f"Client-ID {settings.UNSPLASH_ACCESS_KEY}",
                "Accept-Version": "v1"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = data.get("results", [])
                        if results:
                            # è¿”å›ç¬¬ä¸€å¼ å›¾ç‰‡çš„ regular å°ºå¯¸ URL
                            image_url = results[0]["urls"]["regular"]
                            logger.info(f"âœ… Unsplash æœç´¢æˆåŠŸ: {destination}")
                            return image_url
                    else:
                        logger.warning(f"Unsplash API è¿”å› {response.status}")
        except Exception as e:
            logger.error(f"Unsplash æœç´¢å¤±è´¥: {str(e)}")
        
        return None
    
    def get_amap_static_image(self, location: str, markers: List[Dict] = None, zoom: int = 12) -> str:
        """
        ç”Ÿæˆé«˜å¾·åœ°å›¾é™æ€å›¾ URL
        
        Args:
            location: ä¸­å¿ƒç‚¹åæ ‡ "lng,lat" æˆ–åœ°å€
            markers: æ ‡è®°ç‚¹åˆ—è¡¨ [{"lng": "", "lat": ""}]
            zoom: ç¼©æ”¾çº§åˆ« 3-18
            
        Returns:
            é«˜å¾·é™æ€åœ°å›¾ URL
        """
        base_url = "https://restapi.amap.com/v3/staticmap"
        params = {
            "location": location,
            "zoom": zoom,
            "size": "750*500",
            "scale": 2,
            "key": settings.AMAP_WEB_KEY
        }
        
        # æ·»åŠ æ ‡è®°ç‚¹
        if markers and len(markers) > 0:
            # æ ¼å¼: "æ ‡è®°å¤§å°,æ ‡è®°é¢œè‰²,æ ‡è®°æ ‡ç­¾:ç»åº¦,çº¬åº¦|ç»åº¦,çº¬åº¦"
            markers_str = "mid,0x2CB67D,A:" + "|".join(
                [f"{m.get('lng', '')},{m.get('lat', '')}" for m in markers[:5]]
            )
            params["markers"] = markers_str
        
        url = f"{base_url}?{urlencode(params)}"
        logger.info(f"ğŸ“ ç”Ÿæˆé«˜å¾·é™æ€åœ°å›¾: {location}")
        return url
    
    async def search_xiaohongshu(
        self, 
        keyword: str, 
        max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢å°çº¢ä¹¦æ”»ç•¥
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        url = f"{self.xiaohongshu_base_url}?keyword={keyword}"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=self.headers, timeout=10) as response:
                    if response.status != 200:
                        return []
                    
                    html = await response.text()
                    return self._parse_xiaohongshu_results(html, max_results)
        except Exception as e:
            print(f"å°çº¢ä¹¦æœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_xiaohongshu_results(
        self, 
        html: str, 
        max_results: int
    ) -> List[Dict[str, Any]]:
        """è§£æå°çº¢ä¹¦æœç´¢ç»“æœ"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            # å°çº¢ä¹¦çš„æœç´¢ç»“æœé€šå¸¸åœ¨ç‰¹å®šçš„å®¹å™¨ä¸­
            # ç”±äºå°çº¢ä¹¦æ˜¯åŠ¨æ€åŠ è½½çš„ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´
            # è¿”å›æœç´¢ URL ä¾›å‰ç«¯ä½¿ç”¨
            results.append({
                "type": "xiaohongshu",
                "title": f"å°çº¢ä¹¦æœç´¢ç»“æœ",
                "url": f"{self.xiaohongshu_base_url}?keyword={html[:50]}",
                "source": "xiaohongshu"
            })
        except Exception as e:
            print(f"è§£æå°çº¢ä¹¦ç»“æœå¤±è´¥: {e}")
        
        return results[:max_results]
    
    async def search_web(
        self, 
        query: str, 
        search_type: str = "text",
        max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ DuckDuckGo æœç´¢ç½‘é¡µ
        
        Args:
            query: æœç´¢å…³é”®è¯
            search_type: æœç´¢ç±»å‹ (text/images/videos/news)
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„ ddgs æœç´¢
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None, 
                lambda: search_ddgs(query, search_type, max_results)
            )
            return results
        except Exception as e:
            print(f"DDGS æœç´¢å¤±è´¥: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """
        æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆï¼ˆåŸºç¡€æ£€æŸ¥ï¼‰
        
        Args:
            url: å›¾ç‰‡URL
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if not url:
            return False
        
        # å¿…é¡»æ˜¯HTTPSï¼ˆå¾®ä¿¡å°ç¨‹åºè¦æ±‚ï¼‰
        if not url.startswith("https://"):
            return False
        
        # æ’é™¤å·²çŸ¥æ— æ•ˆçš„åŸŸåï¼ˆæœ‰é˜²ç›—é“¾æˆ–ä¸æ”¯æŒå¤–é“¾ï¼‰
        invalid_domains = [
            "mmbiz.qpic.cn",  # å¾®ä¿¡å›¾ç‰‡
            "kuaizhan.com",   # å¿«ç«™
            "qpic.cn",        # QQå›¾ç‰‡
            "sinaimg.cn",     # æ–°æµªå›¾ç‰‡
            "dmjnb.com",      # 403é˜²ç›—é“¾
            "bdimg.com",      # ç™¾åº¦å›¾ç‰‡
            "baidustatic.com",
            "sogoucdn.com",   # æœç‹—
            "360buyimg.com",  # äº¬ä¸œ
            "alicdn.com",     # é˜¿é‡Œï¼ˆéƒ¨åˆ†æœ‰é˜²ç›—é“¾ï¼‰
            "ctrip.com",      # æºç¨‹
            "mafengwo.net",   # é©¬èœ‚çª
            "duitang.com",    # å †ç³–
            "huaban.com",     # èŠ±ç“£
            "nipic.com",      # æ˜µå›¾ç½‘
            "58pic.com",      # åƒå›¾ç½‘
            "zcool.cn",       # ç«™é…·
        ]
        for domain in invalid_domains:
            if domain in url:
                return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¸¸è§å›¾ç‰‡æ‰©å±•åæˆ–å¯é å›¾ç‰‡æœåŠ¡
        valid_patterns = [
            ".jpg", ".jpeg", ".png", ".webp", ".gif",
            "unsplash.com", "pexels.com", "pixabay.com",
            "cloudfront.net", "amazonaws.com",  # AWS CDN
            "googleusercontent.com",
            "flickr.com", "staticflickr.com",
            "wikimedia.org", "wikipedia.org",
        ]
        url_lower = url.lower()
        return any(pattern in url_lower for pattern in valid_patterns)
    
    async def _verify_image_accessible(self, url: str) -> bool:
        """
        éªŒè¯å›¾ç‰‡URLæ˜¯å¦å¯ä»¥è®¿é—®ï¼ˆHEADè¯·æ±‚ï¼‰
        
        Args:
            url: å›¾ç‰‡URL
            
        Returns:
            æ˜¯å¦å¯è®¿é—®
        """
        import httpx
        try:
            async with httpx.AsyncClient(timeout=8.0, follow_redirects=True) as client:
                response = await client.head(url)
                if response.status_code != 200:
                    return False
                # æ£€æŸ¥Content-Typeæ˜¯å¦ä¸ºå›¾ç‰‡
                content_type = response.headers.get("content-type", "")
                if not any(t in content_type.lower() for t in ["image/", "octet-stream"]):
                    return False
                return True
        except Exception as e:
            print(f"  éªŒè¯å›¾ç‰‡å¤±è´¥: {e}")
            return False
    
    async def search_destination_image(
        self, 
        destination: str,
        location: str = None,
        markers: List[Dict] = None
    ) -> Optional[str]:
        """
        æœç´¢ç›®çš„åœ°å›¾ç‰‡ - å¤šçº§é™çº§ç­–ç•¥
        
        ä¼˜å…ˆçº§:
        1. Unsplash API (é«˜è´¨é‡æ—…è¡Œå›¾ç‰‡)
        2. é«˜å¾·åœ°å›¾é™æ€å›¾ (å¯é çš„åœ°å›¾è§†å›¾)
        3. DDGS æœç´¢ (å¤‡é€‰æ–¹æ¡ˆ)
        
        Args:
            destination: ç›®çš„åœ°åç§°
            location: åœ°å›¾ä¸­å¿ƒç‚¹åæ ‡ "lng,lat"
            markers: åœ°å›¾æ ‡è®°ç‚¹åˆ—è¡¨
            
        Returns:
            å›¾ç‰‡URLï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›None
        """
        logger.info(f"ğŸ–¼ï¸ å¼€å§‹æœç´¢ {destination} çš„å°é¢å›¾ç‰‡...")
        
        # ç­–ç•¥1: å°è¯• Unsplash API
        try:
            unsplash_url = await self.search_unsplash_image(destination)
            if unsplash_url:
                logger.info(f"âœ… ä½¿ç”¨ Unsplash å›¾ç‰‡")
                return unsplash_url
        except Exception as e:
            logger.error(f"Unsplash æœç´¢å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # ç­–ç•¥2: ä½¿ç”¨é«˜å¾·åœ°å›¾é™æ€å›¾
        if location and settings.AMAP_WEB_KEY:
            try:
                amap_url = self.get_amap_static_image(location, markers)
                logger.info(f"âœ… ä½¿ç”¨é«˜å¾·åœ°å›¾é™æ€å›¾")
                return amap_url
            except Exception as e:
                logger.warning(f"é«˜å¾·åœ°å›¾ç”Ÿæˆå¼‚å¸¸: {str(e)}")
        
        # ç­–ç•¥3: DDGS æœç´¢ï¼ˆå¤‡é€‰ï¼‰
        logger.info("âš ï¸ Unsplash å’Œé«˜å¾·åœ°å›¾éƒ½ä¸å¯ç”¨ï¼Œå°è¯• DDGS...")
        try:
            # åªå°è¯•ä¸€æ¬¡æœ€æœ‰æ•ˆçš„æœç´¢è¯
            queries = [
                f"{destination} travel scenery landscape",
            ]
            
            valid_images = []
            total_filtered = 0
            filter_reasons = {}
            
            for idx, query in enumerate(queries, 1):
                try:
                    print(f"  ğŸ” æŸ¥è¯¢ {idx}/{len(queries)}: '{query}'")
                    results = await self.search_web(query, "images", 20)
                    print(f"     DDGSè¿”å› {len(results)} ä¸ªç»“æœ")
                    
                    if results and len(results) > 0:
                        for result in results:
                            # ä¼˜å…ˆä½¿ç”¨imageå­—æ®µï¼Œå…¶æ¬¡æ˜¯thumbnail
                            image_url = result.get("image") or result.get("thumbnail")
                            
                            if not image_url:
                                total_filtered += 1
                                filter_reasons["æ— URL"] = filter_reasons.get("æ— URL", 0) + 1
                                continue
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
                            if not image_url.startswith("https://"):
                                total_filtered += 1
                                filter_reasons["éHTTPS"] = filter_reasons.get("éHTTPS", 0) + 1
                                print(f"     âŒ è¿‡æ»¤(éHTTPS): {image_url[:80]}")
                                continue
                            
                            # æ£€æŸ¥åŸŸåé»‘åå•
                            invalid_domains = [
                                "mmbiz.qpic.cn", "kuaizhan.com", "qpic.cn", "sinaimg.cn",
                                "dmjnb.com", "bdimg.com", "baidustatic.com", "sogoucdn.com",
                                "360buyimg.com", "alicdn.com", "ctrip.com", "mafengwo.net",
                                "duitang.com", "huaban.com", "nipic.com", "58pic.com", "zcool.cn"
                            ]
                            is_blacklisted = False
                            for domain in invalid_domains:
                                if domain in image_url:
                                    total_filtered += 1
                                    filter_reasons[f"é»‘åå•-{domain}"] = filter_reasons.get(f"é»‘åå•-{domain}", 0) + 1
                                    print(f"     âŒ è¿‡æ»¤(é»‘åå•-{domain}): {image_url[:80]}")
                                    is_blacklisted = True
                                    break
                            
                            if is_blacklisted:
                                continue
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ä¿¡æ‰©å±•åæˆ–åŸŸå
                            valid_patterns = [
                                ".jpg", ".jpeg", ".png", ".webp", ".gif",
                                "unsplash.com", "pexels.com", "pixabay.com",
                                "cloudfront.net", "amazonaws.com",
                                "googleusercontent.com", "flickr.com", "staticflickr.com",
                                "wikimedia.org", "wikipedia.org",
                            ]
                            url_lower = image_url.lower()
                            if not any(pattern in url_lower for pattern in valid_patterns):
                                total_filtered += 1
                                filter_reasons["æ— å¯ä¿¡æ ‡è¯†"] = filter_reasons.get("æ— å¯ä¿¡æ ‡è¯†", 0) + 1
                                print(f"     âŒ è¿‡æ»¤(æ— å¯ä¿¡æ ‡è¯†): {image_url[:80]}")
                                continue
                            
                            # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
                            valid_images.append(image_url)
                            print(f"     âœ… å€™é€‰å›¾ç‰‡: {image_url[:80]}")
                            
                            # æ”¶é›†è¶³å¤Ÿå¤šçš„å€™é€‰å›¾ç‰‡åå¼€å§‹éªŒè¯
                            if len(valid_images) >= 5:
                                break
                except Exception as e:
                    print(f"     âŒ æœç´¢å¼‚å¸¸: {e}")
                    continue
                
                # å¦‚æœå·²ç»æœ‰å€™é€‰å›¾ç‰‡ï¼Œå¼€å§‹éªŒè¯
                if valid_images:
                    print(f"  ğŸ“Š è¿‡æ»¤ç»Ÿè®¡: æ€»å…±è¿‡æ»¤ {total_filtered} å¼ ï¼Œé€šè¿‡ {len(valid_images)} å¼ ")
                    if filter_reasons:
                        print(f"     è¿‡æ»¤åŸå› : {filter_reasons}")
                    break
            
            if not valid_images:
                print(f"  âš ï¸ æ‰€æœ‰æŸ¥è¯¢éƒ½æ²¡æœ‰è¿”å›æœ‰æ•ˆå€™é€‰å›¾ç‰‡")
                print(f"  ğŸ“Š è¿‡æ»¤ç»Ÿè®¡: æ€»å…±è¿‡æ»¤ {total_filtered} å¼ ")
                if filter_reasons:
                    print(f"     è¿‡æ»¤åŸå› : {filter_reasons}")
                return None
            
            # éªŒè¯å€™é€‰å›¾ç‰‡
            print(f"  ğŸ” å¼€å§‹éªŒè¯ {len(valid_images)} ä¸ªå€™é€‰å›¾ç‰‡...")
            for idx, image_url in enumerate(valid_images, 1):
                try:
                    print(f"     éªŒè¯ {idx}/{len(valid_images)}: {image_url[:80]}")
                    if await self._verify_image_accessible(image_url):
                        print(f"  âœ… æ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡: {image_url}")
                        return image_url
                    else:
                        print(f"     âŒ å›¾ç‰‡æ— æ³•è®¿é—®(HTTPæ£€æŸ¥å¤±è´¥)")
                except Exception as e:
                    print(f"     âŒ éªŒè¯å¼‚å¸¸: {e}")
                    continue
            
            print(f"  âš ï¸ æœªæ‰¾åˆ° {destination} çš„æœ‰æ•ˆå°é¢å›¾ç‰‡ï¼ˆæ‰€æœ‰å€™é€‰å›¾ç‰‡éªŒè¯å¤±è´¥ï¼‰")
            return None
        except Exception as e:
            print(f"âŒ æœç´¢ç›®çš„åœ°å›¾ç‰‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def search_travel_guides(
        self, 
        destination: str, 
        preferences: List[str] = None
    ) -> Dict[str, Any]:
        """
        ç»¼åˆæœç´¢æ—…è¡Œæ”»ç•¥
        
        Args:
            destination: ç›®çš„åœ°
            preferences: åå¥½æ ‡ç­¾
            
        Returns:
            ç»¼åˆæœç´¢ç»“æœ
        """
        prefs = " ".join(preferences) if preferences else ""
        query = f"{destination} æ—…æ¸¸æ”»ç•¥ {prefs}".strip()
        
        # å¹¶è¡Œæœç´¢å¤šä¸ªæ¥æº
        tasks = [
            self.search_web(query, "text", 5),
            self.search_web(f"{destination} ç¾é£Ÿæ¨è", "text", 5),
            self.search_web(f"{destination} æ™¯ç‚¹", "text", 5),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return {
            "general": results[0] if not isinstance(results[0], Exception) else [],
            "food": results[1] if not isinstance(results[1], Exception) else [],
            "attractions": results[2] if not isinstance(results[2], Exception) else [],
            "xiaohongshu_url": f"{self.xiaohongshu_base_url}?keyword={destination}æ”»ç•¥"
        }


# å…¨å±€æœç´¢æœåŠ¡å®ä¾‹
search_service = SearchService()
